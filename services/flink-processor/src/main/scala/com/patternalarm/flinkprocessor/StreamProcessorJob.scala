package com.patternalarm.flinkprocessor

import com.patternalarm.flinkprocessor.config.Config
import com.patternalarm.flinkprocessor.model._
import com.patternalarm.flinkprocessor.processor._
import com.patternalarm.flinkprocessor.serialization.InstantSerializer
import com.patternalarm.flinkprocessor.sink.FraudAlertSink
import com.patternalarm.flinkprocessor.utils.JsonUtils
import org.apache.flink.api.common.eventtime.{SerializableTimestampAssigner, WatermarkStrategy}
import org.apache.flink.api.common.functions.{FilterFunction, MapFunction}
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.datastream.{AsyncDataStream, DataStream}
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer

import java.time.{Duration, Instant}
import java.util.Properties
import java.util.concurrent.TimeUnit

class StreamProcessorJob(
  envProvider: () => StreamExecutionEnvironment,
  kafkaSourceProvider: StreamExecutionEnvironment => DataStream[String],
  fraudScoringAsyncFunction: FraudScoringAsyncFunction,
  alertSink: FraudAlertSink
) {

  def this() = this(
    StreamProcessorJob.setupEnvironment,
    StreamProcessorJob.createKafkaSource,
    new FraudScoringAsyncFunction(Config.FastApi.url),
    StreamProcessorJob.createAlertSink()
  )

  def run(args: Array[String] = Array.empty): Unit = {
    println("ðŸ“‹ [STEP 1] Printing configuration summary...")
    Config.printSummary()

    println("ðŸ“‹ [STEP 2] Setting up Flink environment...")
    val env = envProvider()
    println("âœ… Flink environment created successfully")

    println("ðŸ“‹ [STEP 3] Creating Kafka source...")
    val kafkaSource = kafkaSourceProvider(env)
    println("âœ… Kafka source created successfully")

    println("ðŸ“‹ [STEP 4] Building processing pipeline...")
    
    println("  â””â”€ Adding JSON parser...")
    val parsedStream = kafkaSource.map(new TransactionJsonParser)
    println("  â””â”€ Adding null filter...")
    val filteredStream = parsedStream.filter(_ != null)
    println("  â””â”€ Adding watermark strategy...")
    val watermarkedStream = filteredStream.assignTimestampsAndWatermarks(createWatermarkStrategy)
    println("  â””â”€ Adding keyBy operation...")
    val keyedStream = watermarkedStream.keyBy((event: TransactionEvent) => event.actorId)
    println("  â””â”€ Adding tumbling window...")
    val windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.minutes(Config.Flink.Windowing.sizeMinutes)))
    println("  â””â”€ Adding window function...")
    val aggregates = windowedStream.apply(new TransactionWindowFunction()).map(new AggregateLogger)
    println("âœ… Windowing pipeline configured")

    println("ðŸ“‹ [STEP 5] Adding async fraud scoring...")
    val scoredStream = AsyncDataStream.unorderedWait(
      aggregates,
      fraudScoringAsyncFunction,
      Config.FastApi.timeoutMs,
      TimeUnit.MILLISECONDS,
      Config.FastApi.maxConcurrentRequests
    ).asInstanceOf[DataStream[(TimedWindowAggregate, PredictResponse)]]
    println("âœ… Fraud scoring configured")

    println("ðŸ“‹ [STEP 6] Adding filtering and alerting...")
    scoredStream
      .map(new ScoreLogger)
      .filter(new HighRiskFilter)
      .map(new AlertWithTransactionsBuilder)
      .map(new AlertLogger)
      .addSink(alertSink)
    println("âœ… Complete pipeline configured")

    println("âœ… [STEP 7] Starting Flink job execution...")
    println("â³ Waiting for data from Kafka topic: " + Config.Kafka.topic)
    println("â³ Consumer group: " + Config.Kafka.groupId)
    
    try {
      env.execute("PatternAlarm Fraud Detection Pipeline")
      println("âœ… Flink job completed successfully")
    } catch {
      case e: Exception =>
        System.err.println("âŒ ERROR: Flink job failed!")
        System.err.println(s"âŒ Exception: ${e.getClass.getName}")
        System.err.println(s"âŒ Message: ${e.getMessage}")
        e.printStackTrace()
        throw e
    }
  }

  private def createWatermarkStrategy: WatermarkStrategy[TransactionEvent] =
    WatermarkStrategy
      .forBoundedOutOfOrderness[TransactionEvent](
        Duration.ofSeconds(Config.Flink.Windowing.latenessSeconds)
      )
      .withTimestampAssigner(new SerializableTimestampAssigner[TransactionEvent] {
        override def extractTimestamp(event: TransactionEvent, recordTimestamp: Long): Long =
          event.timestamp.toEpochMilli
      })
}

object StreamProcessorJob {

  def main(args: Array[String]): Unit = {
    println("ðŸš€ Starting PatternAlarm Fraud Detection Pipeline...")
    println("ðŸ”§ Initializing health check server...")
    startHealthCheck()
    println("ðŸ”§ Creating StreamProcessorJob instance...")
    
    try {
      new StreamProcessorJob().run(args)
    } catch {
      case e: Exception =>
        System.err.println("âŒ FATAL ERROR in main!")
        System.err.println(s"âŒ Exception: ${e.getClass.getName}")
        System.err.println(s"âŒ Message: ${e.getMessage}")
        e.printStackTrace()
        System.exit(1)
    }
  }

  // ========== Factory Methods ==========

  def setupEnvironment(): StreamExecutionEnvironment = {
    println("ðŸ”§ Creating Flink execution environment...")
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    
    println(s"ðŸ”§ Enabling checkpointing: ${Config.Flink.checkpointingIntervalMs}ms")
    env.enableCheckpointing(Config.Flink.checkpointingIntervalMs)

    println("ðŸ”§ Registering Kryo serializer for Instant...")
    env.getConfig.registerTypeWithKryoSerializer(
      classOf[Instant],
      classOf[InstantSerializer]
    )

    println("âœ… Flink environment setup complete")
    env
  }

  def createKafkaSource(env: StreamExecutionEnvironment): DataStream[String] = {
    println("ðŸ”Œ Setting up Kafka consumer...")
    
    val kafkaProperties = new Properties()
    kafkaProperties.setProperty("bootstrap.servers", Config.Kafka.bootstrapServers)
    kafkaProperties.setProperty("group.id", Config.Kafka.groupId)
    kafkaProperties.setProperty("auto.offset.reset", Config.Kafka.autoOffsetReset)
    
    // Add timeouts to fail fast
    kafkaProperties.setProperty("session.timeout.ms", "30000")
    kafkaProperties.setProperty("request.timeout.ms", "40000")
    kafkaProperties.setProperty("metadata.max.age.ms", "30000")
    
    println(s"ðŸ”Œ Kafka Bootstrap Servers: ${Config.Kafka.bootstrapServers}")
    println(s"ðŸ”Œ Kafka Topic: ${Config.Kafka.topic}")
    println(s"ðŸ”Œ Kafka Group ID: ${Config.Kafka.groupId}")
    println(s"ðŸ”Œ Auto Offset Reset: ${Config.Kafka.autoOffsetReset}")

    try {
      println("ðŸ”Œ Creating FlinkKafkaConsumer instance...")
      val consumer = new FlinkKafkaConsumer[String](
        Config.Kafka.topic,
        new SimpleStringSchema(),
        kafkaProperties
      )
      
      println("âœ… FlinkKafkaConsumer created successfully")
      println("ðŸ”Œ Adding Kafka source to environment...")
      
      val source = env.addSource(consumer)
      println("âœ… Kafka source added to environment")
      
      source
    } catch {
      case e: Exception =>
        System.err.println("âŒ ERROR: Failed to create Kafka source!")
        System.err.println(s"âŒ Exception: ${e.getClass.getName}")
        System.err.println(s"âŒ Message: ${e.getMessage}")
        e.printStackTrace()
        throw e
    }
  }

  def createAlertSink(): FraudAlertSink = {
    println("ðŸ’¾ Creating FraudAlertSink...")
    println(s"ðŸ’¾ Database URL: ${Config.Database.url}")
    println(s"ðŸ’¾ Database User: ${Config.Database.user}")
    
    try {
      val sink = new FraudAlertSink(
        Config.Database.url,
        Config.Database.user,
        Config.Database.password
      )
      println("âœ… FraudAlertSink created successfully")
      sink
    } catch {
      case e: Exception =>
        System.err.println("âŒ ERROR: Failed to create FraudAlertSink!")
        System.err.println(s"âŒ Exception: ${e.getClass.getName}")
        System.err.println(s"âŒ Message: ${e.getMessage}")
        e.printStackTrace()
        throw e
    }
  }

  def startHealthCheck(): Unit = {
    val healthCheckThread = new Thread(() => {
      try {
        val serverSocket = new java.net.ServerSocket(8081)
        println("âœ… Health check server listening on port 8081")
        while (true) {
          val socket = serverSocket.accept()
          val out = new java.io.PrintWriter(socket.getOutputStream, true)
          out.println("HTTP/1.1 200 OK")
          out.println("Content-Type: text/plain")
          out.println("Content-Length: 2")
          out.println()
          out.println("OK")
          out.flush()
          socket.close()
        }
      } catch {
        case e: Exception => 
          System.err.println(s"âŒ Health check server error: ${e.getMessage}")
          e.printStackTrace()
      }
    })
    healthCheckThread.setDaemon(true)
    healthCheckThread.start()
  }

  // ========== Domain Logic ==========

  private[flinkprocessor] def parseJson(json: String): TransactionEvent =
    try {
      val event = JsonUtils.fromJson[TransactionEvent](json)
      println(s"âœ… Parsed transaction: actorId=${event.actorId}, domain=${event.domain}, amount=${event.amount}")
      event
    } catch {
      case e: Exception =>
        System.err.println(s"âš ï¸  Failed to parse JSON: ${e.getMessage}")
        System.err.println(s"âš ï¸  Raw JSON: $json")
        null
    }

  private[flinkprocessor] def isHighRisk(tuple: (TimedWindowAggregate, PredictResponse)): Boolean = {
    val (agg, response) = tuple
    val isHighRisk = response.fraudScore >= Config.Flink.FraudDetection.scoreThreshold
    if (isHighRisk) {
      println(s"ðŸš¨ HIGH RISK DETECTED: actor=${agg.actorId}, score=${response.fraudScore}")
    } else {
      println(s"âœ“ Low risk: actor=${agg.actorId}, score=${response.fraudScore}")
    }
    isHighRisk
  }

  private[flinkprocessor] def buildAlertWithTransactions(
    aggregate: TimedWindowAggregate,
    response: PredictResponse
  ): (Alert, Seq[TransactionEvent]) = {

    println(s"ðŸ“ Building alert for actor=${aggregate.actorId}, score=${response.fraudScore}")

    val patterns = aggregate.transactions
      .filter(_.isFraud)
      .map(_.pattern)
      .distinct
      .filter(_ != "regular")

    val firstTx = aggregate.transactions.headOption
    val windowSeconds = Duration.between(aggregate.windowStart, aggregate.windowEnd).getSeconds

    val alert = Alert(
      alertId = 0,
      alertType = determineAlertType(aggregate),
      domain = aggregate.domain,
      actorId = aggregate.actorId,
      severity = determineSeverity(response.fraudScore),
      fraudScore = response.fraudScore,
      transactionCount = aggregate.transactionCount,
      totalAmount = aggregate.totalAmount,
      firstSeen = aggregate.windowStart,
      lastSeen = aggregate.windowEnd,
      windowSeconds = Some(windowSeconds),
      baselineAvg = Some(aggregate.totalAmount / aggregate.transactionCount),
      patternsDetected = Some(patterns),
      confidence = Some(response.fraudScore),
      modelVersion = Some(response.modelVersion),
      inferenceTimeMs = Some(response.inferenceTimeMs),
      playerId = firstTx.flatMap(_.playerId),
      gameId = firstTx.flatMap(_.gameId),
      itemType = firstTx.flatMap(_.itemType),
      itemName = firstTx.flatMap(_.itemName),
      sessionLengthSec = firstTx.flatMap(_.sessionLengthSec),
      accountFrom = firstTx.flatMap(_.accountFrom),
      accountTo = firstTx.flatMap(_.accountTo),
      transferType = firstTx.flatMap(_.transferType),
      countryFrom = firstTx.flatMap(_.countryFrom),
      countryTo = firstTx.flatMap(_.countryTo),
      purpose = firstTx.flatMap(_.purpose),
      userId = firstTx.flatMap(_.userId),
      cartItems = firstTx.flatMap(_.cartItems),
      shippingAddress = firstTx.flatMap(_.shippingAddress),
      billingAddress = firstTx.flatMap(_.billingAddress),
      deviceFingerprint = firstTx.flatMap(_.deviceFingerprint),
      sessionDurationSec = firstTx.flatMap(_.sessionDurationSec),
      paymentMethod = firstTx.flatMap(_.paymentMethod),
      deviceId = firstTx.flatMap(_.deviceId),
      ipAddress = Some(firstTx.map(_.ipAddress).getOrElse("unknown"))
    )

    println(s"âœ… Alert built: type=${alert.alertType}, severity=${alert.severity}")
    (alert, aggregate.transactions)
  }

  private[flinkprocessor] def determineAlertType(aggregate: TimedWindowAggregate): String = {
    val fraudPatterns = aggregate.transactions
      .filter(_.isFraud)
      .map(_.pattern)

    if (fraudPatterns.isEmpty) {
      "suspicious_activity"
    } else {
      fraudPatterns
        .groupBy(identity)
        .maxBy(_._2.size)
        ._1
        .replace("fraud_", "")
    }
  }

  private[flinkprocessor] def determineSeverity(fraudScore: Int): String = fraudScore match {
    case s if s >= 90 => "CRITICAL"
    case s if s >= 75 => "HIGH"
    case s if s >= 60 => "MEDIUM"
    case _ => "LOW"
  }
}

// ========== Serializable Operators ==========

@SerialVersionUID(100L)
class TransactionJsonParser extends MapFunction[String, TransactionEvent] {
  override def map(json: String): TransactionEvent = {
    println(s"ðŸ“¥ Received message from Kafka (${json.length} chars)")
    StreamProcessorJob.parseJson(json)
  }
}

@SerialVersionUID(101L)
class AggregateLogger extends MapFunction[TimedWindowAggregate, TimedWindowAggregate] {
  override def map(agg: TimedWindowAggregate): TimedWindowAggregate = {
    println(s"ðŸ“Š Window: actor=${agg.actorId}, txns=${agg.transactionCount}, amount=${agg.totalAmount}, domain=${agg.domain}")
    agg
  }
}

@SerialVersionUID(102L)
class ScoreLogger
    extends MapFunction[(TimedWindowAggregate, PredictResponse), (TimedWindowAggregate, PredictResponse)] {
  override def map(tuple: (TimedWindowAggregate, PredictResponse)): (TimedWindowAggregate, PredictResponse) = {
    val (agg, response) = tuple
    println(s"ðŸŽ¯ Score: actor=${agg.actorId}, fraud_score=${response.fraudScore}, model=${response.modelVersion}")
    tuple
  }
}

@SerialVersionUID(103L)
class AlertLogger extends MapFunction[(Alert, Seq[TransactionEvent]), (Alert, Seq[TransactionEvent])] {
  override def map(tuple: (Alert, Seq[TransactionEvent])): (Alert, Seq[TransactionEvent]) = {
    val (alert, txs) = tuple
    println(s"ðŸš¨ ALERT: ${alert.severity} - actor=${alert.actorId}, score=${alert.fraudScore}, type=${alert.alertType}, txCount=${txs.length}")
    tuple
  }
}

@SerialVersionUID(104L)
class HighRiskFilter extends FilterFunction[(TimedWindowAggregate, PredictResponse)] {
  override def filter(tuple: (TimedWindowAggregate, PredictResponse)): Boolean =
    StreamProcessorJob.isHighRisk(tuple)
}

@SerialVersionUID(105L)
class AlertWithTransactionsBuilder
    extends MapFunction[(TimedWindowAggregate, PredictResponse), (Alert, Seq[TransactionEvent])] {
  override def map(tuple: (TimedWindowAggregate, PredictResponse)): (Alert, Seq[TransactionEvent]) = {
    val (aggregate, response) = tuple
    StreamProcessorJob.buildAlertWithTransactions(aggregate, response)
  }
}
